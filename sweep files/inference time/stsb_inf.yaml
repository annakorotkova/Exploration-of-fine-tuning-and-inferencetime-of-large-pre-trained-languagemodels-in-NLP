program: /home/ubuntu/transformers/examples/text-classification/run_glue.py
command:
  - ${env}
  - python3
  - ${program}
  - "--do_train" 
  - "--do_eval" 
  - "--evaluate_during_training" 
  - "--overwrite_output_dir"
  - ${args}
method: grid
metric:
  name: eval_acc
  goal: maximize
parameters:
  #
  # parameters to be optimized over
  #
  learning_rate:
    value: 2e-5   # see Appendix of ALBERT paper
  per_device_eval_batch_size:
    values: [8, 16, 32]
  max_seq_length:
    values: [128, 256, 512]
  model_name_or_path:
    values: ["bert-base-uncased", "bert-base-cased", 
    "distilbert-base-uncased",  "distilbert-base-cased",
    "roberta-base", 
    "albert-base-v1",
    "xlnet-base-cased"]
  #
  # fixed parameters
  #
  task_name: 
    value: STS-B
  data_dir: 
    value: /home/ubuntu/STS-B    #### downloaded with download_glue_data.py (python3 download_glue_data.py)
  output_dir: 
    value: /tmp/STS-B/
  per_device_train_batch_size:
    value: 32
  num_train_epochs:
    value: 1
  logging_steps:
    value: 1000 # default is 500; set higher values, otherwise the memory is full
  weight_decay:
    value: .01
  adam_epsilon:
    value: 1e-6
  finetuning_iters:
    value: 1      # to receive the inference time, there is no need to iterate the finetuning multiple times