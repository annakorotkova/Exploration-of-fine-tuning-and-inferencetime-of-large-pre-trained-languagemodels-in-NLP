{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLUE_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP36/zg76B/aiJzUp5GAUJB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxgFNfQbqnW",
        "colab_type": "text"
      },
      "source": [
        "Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4iJNG3CcEks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Y1Gik3cFYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_data = drive.CreateFile({'id':'1AlbO4pMgbCAZO2XyGyKXsdWszyj-xMHx'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz0-Ov4mboJX",
        "colab_type": "code",
        "outputId": "af45c8e6-9abd-491e-ab2c-9df1c72dd3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Loading GLUE datasets notebook\n",
        "load_data.GetContentFile('Load_GLUE.ipynb')\n",
        "\n",
        "# run Load_GLUE notebook\n",
        "%run Load_GLUE.ipynb # this notebook downloads and extracts data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting SNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiouTltQUS5o",
        "colab_type": "code",
        "outputId": "f5c12ae5-f61d-4589-b3ad-a1ef439745cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# names of respective datasets\n",
        "data_names = ['CoLA', 'MNLI', 'MRPC', 'QNLI', 'QQP', 'RTE', 'SST-2', 'STS-B', 'WNLI']\n",
        "# 'SNLI' also included, but not explicitly stated in GLUE benchmark --> not regarded until now\n",
        "\n",
        "\n",
        "# define dictionary to save column names for each dataset\n",
        "column_names = {}\n",
        "# to store all datasets in one dict\n",
        "datasets = {}\n",
        "\n",
        "# Load the datasets into a pandas dataframe.\n",
        "for i in data_names:\n",
        "  # for CoLA dataset, column names need to be added\n",
        "  if i == 'CoLA':\n",
        "    datasets[i] = pd.read_csv(\"/content/drive/My Drive/Master Thesis/{0}/train.tsv\".format(i), \n",
        "                    delimiter='\\\\t', quotechar='\"', \n",
        "                    names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "  elif i == 'MRPC':\n",
        "    datasets[i] = pd.read_csv(\"/content/drive/My Drive/Master Thesis/{0}/train.tsv\".format(i), \n",
        "                    delimiter='\\\\t', quotechar='\"', header = 1, names=['Quality', '#1 ID', '#2 ID', \n",
        "                                                                       '#1 String', '#2 String'])\n",
        "  else:\n",
        "    datasets[i] = pd.read_csv(\"/content/drive/My Drive/Master Thesis/{0}/train.tsv\".format(i), \n",
        "                    delimiter = '\\\\t', quotechar = '\"')\n",
        "  # Report the number of sentences.\n",
        "  print('Number of training sentences: {:,}\\n'.format(datasets[i].shape[0]))\n",
        "  # Display 5 random rows from the data.\n",
        "  print(datasets[i].sample(10))\n",
        "  # save column names for each dataset in dict\n",
        "  column_names[i] = datasets[i].columns\n",
        "\n",
        "### in order to work with less data, sample data from datasets\n",
        "for i in data_names:\n",
        "  if datasets[i].shape[0] >= 1000:\n",
        "    datasets[i] = datasets[i].sample(1000)\n",
        "  else:\n",
        "    datasets[i] = datasets[i].sample(datasets[i].shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n",
            "     sentence_source  ...                                           sentence\n",
            "5166            ks08  ...   I believe it to be the switch that is defective.\n",
            "3616            ks08  ...  It was several young students that the policem...\n",
            "6840            m_02  ...                   I forwarded Winifred the letter.\n",
            "1764            r-67  ...  Wilt is taller than I imagine anybody would ev...\n",
            "4729            ks08  ...  This bed was surely slept in by a huge guy las...\n",
            "4110            ks08  ...                           We were glad what to do.\n",
            "7851            ad03  ...                                 Can he will do it?\n",
            "2611            l-93  ...   Lora buttered at the toast with unsalted butter.\n",
            "2412            l-93  ...   The contractor will build a house for $ 100,000.\n",
            "2136            l-93  ...            Carla poured the pitcher with lemonade.\n",
            "\n",
            "[10 rows x 4 columns]\n",
            "Number of training sentences: 392,702\n",
            "\n",
            "         index  promptID  ...         label1     gold_label\n",
            "269872  269872    106754  ...     entailment     entailment\n",
            "124921  124921     63137  ...     entailment     entailment\n",
            "382780  382780     97294  ...     entailment     entailment\n",
            "177933  177933     54367  ...     entailment     entailment\n",
            "364909  364909     42766  ...        neutral        neutral\n",
            "350102  350102     87566  ...     entailment     entailment\n",
            "307906  307906     26100  ...        neutral        neutral\n",
            "67559    67559     61374  ...  contradiction  contradiction\n",
            "68012    68012      3137  ...        neutral        neutral\n",
            "105857  105857     94503  ...        neutral        neutral\n",
            "\n",
            "[10 rows x 12 columns]\n",
            "Number of training sentences: 3,667\n",
            "\n",
            "      Quality  ...                                          #2 String\n",
            "2562        1  ...  About 18 percent of men taking finasteride dev...\n",
            "361         1  ...  Meanwhile, Hynix Semiconductors voiced strong ...\n",
            "2695        1  ...  Launched from space shuttle Atlantis (news - w...\n",
            "2605        0  ...          Both were taken to Boston Medical Center.\n",
            "938         0  ...  The dollar pushed as high as $1.1115 to the eu...\n",
            "512         1  ...  He admits that the law \"has several weaknesses...\n",
            "791         0  ...  LendingTree matches borrowers via the Internet...\n",
            "120         1  ...  Apple noted that half the songs were purchased...\n",
            "2917        0  ...  Staff writers Brandon Formby and Colleen McCai...\n",
            "1936        1  ...  I mean, these are violent surgeries, and I wan...\n",
            "\n",
            "[10 rows x 5 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 104,743\n",
            "\n",
            "       index  ...           label\n",
            "62248  62248  ...      entailment\n",
            "67755  67755  ...      entailment\n",
            "72783  72783  ...  not_entailment\n",
            "59721  59721  ...  not_entailment\n",
            "73525  73525  ...      entailment\n",
            "5974    5974  ...      entailment\n",
            "33510  33510  ...      entailment\n",
            "99620  99620  ...      entailment\n",
            "30793  30793  ...      entailment\n",
            "25190  25190  ...  not_entailment\n",
            "\n",
            "[10 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 363,870\n",
            "\n",
            "            id  ... is_duplicate\n",
            "91751   307118  ...          0.0\n",
            "330375    2560  ...          0.0\n",
            "231484  136722  ...          1.0\n",
            "89709   232263  ...          0.0\n",
            "92481   309648  ...          1.0\n",
            "14643   202442  ...          1.0\n",
            "353922   89259  ...          0.0\n",
            "82387   237335  ...          1.0\n",
            "285688   55780  ...          0.0\n",
            "180898  310011  ...          0.0\n",
            "\n",
            "[10 rows x 6 columns]\n",
            "Number of training sentences: 2,490\n",
            "\n",
            "      index  ...           label\n",
            "1569   1569  ...  not_entailment\n",
            "1896   1896  ...      entailment\n",
            "2125   2125  ...      entailment\n",
            "0         0  ...  not_entailment\n",
            "761     761  ...  not_entailment\n",
            "2183   2183  ...      entailment\n",
            "101     101  ...  not_entailment\n",
            "2485   2485  ...  not_entailment\n",
            "787     787  ...      entailment\n",
            "1338   1338  ...      entailment\n",
            "\n",
            "[10 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 67,349\n",
            "\n",
            "                                                sentence  label\n",
            "59109                     lumpy as two-day old porridge       0\n",
            "63673                 this is n't worth sitting through       0\n",
            "17112  much about the film , including some of its ca...      1\n",
            "34171                first-class , thoroughly involving       1\n",
            "13130                            a dark-as-pitch comedy       0\n",
            "55117  a delightful surprise because despite all the ...      1\n",
            "58453  walk out of the good girl with mixed emotions --       0\n",
            "43275                      's undeniably hard to follow       0\n",
            "64707  anything except that the chelsea hotel today i...      0\n",
            "18506            and that is where ararat went astray .       0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 5,749\n",
            "\n",
            "      index  ... score\n",
            "2944   2944  ...  2.75\n",
            "4433   4433  ...  4.00\n",
            "4194   4194  ...  0.60\n",
            "1109   1109  ...  1.20\n",
            "3816   3816  ...  1.60\n",
            "2341   2341  ...  2.00\n",
            "3211   3211  ...  3.40\n",
            "1797   1797  ...  0.00\n",
            "2421   2421  ...  2.20\n",
            "2136   2136  ...  4.40\n",
            "\n",
            "[10 rows x 10 columns]\n",
            "Number of training sentences: 635\n",
            "\n",
            "     index  ... label\n",
            "140    140  ...     0\n",
            "156    156  ...     1\n",
            "434    434  ...     1\n",
            "70      70  ...     1\n",
            "397    397  ...     0\n",
            "47      47  ...     0\n",
            "473    473  ...     1\n",
            "589    589  ...     0\n",
            "39      39  ...     1\n",
            "69      69  ...     0\n",
            "\n",
            "[10 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izgiKkFsnfKA",
        "colab_type": "code",
        "outputId": "52ecc7f2-26fb-46de-af6b-680ea3466146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# save colnames of respective datasets in dictionary\n",
        "colnames = {}\n",
        "for i in data_names:\n",
        "    colnames[i] = datasets[i].columns.values\n",
        "\n",
        "print(colnames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CoLA': array(['sentence_source', 'label', 'label_notes', 'sentence'],\n",
            "      dtype=object), 'MNLI': array(['index', 'promptID', 'pairID', 'genre', 'sentence1_binary_parse',\n",
            "       'sentence2_binary_parse', 'sentence1_parse', 'sentence2_parse',\n",
            "       'sentence1', 'sentence2', 'label1', 'gold_label'], dtype=object), 'MRPC': array(['Quality', '#1 ID', '#2 ID', '#1 String', '#2 String'],\n",
            "      dtype=object), 'QNLI': array(['index', 'question', 'sentence', 'label'], dtype=object), 'QQP': array(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'],\n",
            "      dtype=object), 'RTE': array(['index', 'sentence1', 'sentence2', 'label'], dtype=object), 'SST-2': array(['sentence', 'label'], dtype=object), 'STS-B': array(['index', 'genre', 'filename', 'year', 'old_index', 'source1',\n",
            "       'source2', 'sentence1', 'sentence2', 'score'], dtype=object), 'WNLI': array(['index', 'sentence1', 'sentence2', 'label'], dtype=object)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNgu8X8r_1pb",
        "colab_type": "code",
        "outputId": "7cb2f565-1e25-4ad4-b2f6-d635ee88b110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# print all possible values of labels\n",
        "for i in data_names:\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    if datasets[i].columns.values[j] == 'label':\n",
        "      print(i + \" labels contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].label)) \n",
        "    # MNLI contains variable gold_label\n",
        "    elif datasets[i].columns.values[j] == 'gold_label':\n",
        "      print(i + \" labels (gold_label) contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].gold_label)) \n",
        "    # QQP contains variable is_duplicate\n",
        "    elif datasets[i].columns.values[j] == 'is_duplicate':\n",
        "      print(i + \" labels (is_duplicate) contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].is_duplicate)) \n",
        "    # MRPC contains variable Quality\n",
        "    elif datasets[i].columns.values[j] == 'Quality':\n",
        "      print(i + \" labels (is_duplicate) contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].Quality)) \n",
        "\n",
        "# unique values of labels: [0 1] and ['not_entailment' 'entailment']\n",
        "# unique values of gold_label (MNLI): ['neutral' 'entailment' 'contradiction']\n",
        "# unique values of is_duplicate (QQP): [ 0.  1. nan]\n",
        "# unique values of Quality (MRPC): [0 1]\n",
        "# STS-B contains scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CoLA labels contain following possible values of labels: \n",
            "[1 0]\n",
            "MNLI labels (gold_label) contain following possible values of labels: \n",
            "['neutral' 'entailment' 'contradiction']\n",
            "MRPC labels (is_duplicate) contain following possible values of labels: \n",
            "[0 1]\n",
            "QNLI labels contain following possible values of labels: \n",
            "['not_entailment' 'entailment']\n",
            "QQP labels (is_duplicate) contain following possible values of labels: \n",
            "[ 0.  1. nan]\n",
            "RTE labels contain following possible values of labels: \n",
            "['not_entailment' 'entailment']\n",
            "SST-2 labels contain following possible values of labels: \n",
            "[0 1]\n",
            "WNLI labels contain following possible values of labels: \n",
            "[1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9_KL77HW0Wb",
        "colab_type": "code",
        "outputId": "91eef161-4636-4d44-e16c-b6787e99c91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# modifying labels for MNLI, QNLI and RTE (before: string labels), \n",
        "# so that the model can work with them (model cannot work with strings)\n",
        "labels = {}\n",
        "mnli_qnli_rte = ['MNLI', 'QNLI', 'RTE']\n",
        "\n",
        "#### Encode labels that have string values as int values, so that the model can\n",
        "#### cope with it\n",
        "\n",
        "# create empty ndarrays for QNLI and RTE labels\n",
        "for i in mnli_qnli_rte:\n",
        "  # for each colums in respective datasets\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    if datasets[i].columns.values[j] == 'label':\n",
        "      labels[i] = np.empty(shape=datasets[i].shape[0], dtype = int, order='C')\n",
        "        # check if the lengths of the empty arrays equal lengths of the respective datasets\n",
        "    elif datasets[i].columns.values[j] == 'gold_label':\n",
        "      labels[i] = np.empty(shape=datasets[i].shape[0], dtype = int, order='C')\n",
        "  print(len(labels[i]) == datasets[i].shape[0])  # output: TRUE\n",
        "\n",
        "# encode labels for MNLI, QNLI and RTE\n",
        "for i in mnli_qnli_rte:\n",
        "  # for each colums in respective datasets\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    counter = 0\n",
        "    # for column 'label'\n",
        "    if datasets[i].columns.values[j] == 'label':\n",
        "      # for each label in label (for QNLI & RTE)\n",
        "      for label in datasets[i].label:\n",
        "        # encode 'not_entailment' as 0\n",
        "        if label == 'not_entailment':\n",
        "          labels[i][counter] = 0\n",
        "        # encode 'entailment' as 1\n",
        "        elif label == 'entailment':\n",
        "          labels[i][counter] = 1\n",
        "        counter += 1\n",
        "    elif datasets[i].columns.values[j] == 'gold_label':\n",
        "      # for each label in gold_label (MNLI)\n",
        "      for label in datasets[i].gold_label:\n",
        "        # encode 'neutral' as 0\n",
        "        if label == 'neutral':\n",
        "          labels[i][counter] = 0\n",
        "        # encode 'entailment' as 1\n",
        "        elif label == 'entailment':\n",
        "          labels[i][counter] = 1\n",
        "        # encode 'contradiction' as -1\n",
        "        elif label == 'contradiction':\n",
        "          labels[i][counter] = -1\n",
        "        counter += 1\n",
        "  # print unique labels and respective count of each unique label\n",
        "  print(np.unique(labels[i], return_counts=True))\n",
        "\n",
        "# add new column to dataset (with coded labels)\n",
        "for i in mnli_qnli_rte:\n",
        "  datasets[i]['label_coded'] = labels[i]\n",
        "  print(np.unique(datasets[i].label_coded))\n",
        "  print(datasets[i].sample(5))\n",
        "\n",
        "# rename columns of MRPC (problem with # in python)\n",
        "datasets['MRPC'].columns = ['Quality', 'ID1', 'ID2', 'String1', 'String2']\n",
        "\n",
        "# save colnames after encoding labels (so that encoded labels are also included)\n",
        "colnames = {}\n",
        "for i in data_names:\n",
        "    colnames[i] = datasets[i].columns.values\n",
        "\n",
        "print(colnames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "(array([-1,  0,  1]), array([130903, 130900, 130899]))\n",
            "(array([0, 1]), array([52366, 52377]))\n",
            "(array([0, 1]), array([1241, 1249]))\n",
            "[-1  0  1]\n",
            "         index  promptID   pairID  ...         label1     gold_label label_coded\n",
            "184965  184965    136669  136669e  ...     entailment     entailment           1\n",
            "168825  168825    107256  107256n  ...        neutral        neutral           0\n",
            "334210  334210    124235  124235n  ...        neutral        neutral           0\n",
            "30901    30901     53651   53651n  ...        neutral        neutral           0\n",
            "249052  249052     51002   51002c  ...  contradiction  contradiction          -1\n",
            "\n",
            "[5 rows x 13 columns]\n",
            "[0 1]\n",
            "       index  ... label_coded\n",
            "45426  45426  ...           0\n",
            "90646  90646  ...           1\n",
            "51473  51473  ...           1\n",
            "14572  14572  ...           1\n",
            "73156  73156  ...           0\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "[0 1]\n",
            "      index  ... label_coded\n",
            "5         5  ...           1\n",
            "1992   1992  ...           0\n",
            "413     413  ...           1\n",
            "1162   1162  ...           0\n",
            "2078   2078  ...           0\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "{'CoLA': array(['sentence_source', 'label', 'label_notes', 'sentence'],\n",
            "      dtype=object), 'MNLI': array(['index', 'promptID', 'pairID', 'genre', 'sentence1_binary_parse',\n",
            "       'sentence2_binary_parse', 'sentence1_parse', 'sentence2_parse',\n",
            "       'sentence1', 'sentence2', 'label1', 'gold_label', 'label_coded'],\n",
            "      dtype=object), 'MRPC': array(['Quality', 'ID1', 'ID2', 'String1', 'String2'], dtype=object), 'QNLI': array(['index', 'question', 'sentence', 'label', 'label_coded'],\n",
            "      dtype=object), 'QQP': array(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'],\n",
            "      dtype=object), 'RTE': array(['index', 'sentence1', 'sentence2', 'label', 'label_coded'],\n",
            "      dtype=object), 'SST-2': array(['sentence', 'label'], dtype=object), 'STS-B': array(['index', 'genre', 'filename', 'year', 'old_index', 'source1',\n",
            "       'source2', 'sentence1', 'sentence2', 'score'], dtype=object), 'WNLI': array(['index', 'sentence1', 'sentence2', 'label'], dtype=object)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKTGD1AiALSJ",
        "colab_type": "code",
        "outputId": "2ab65d15-b1ea-4e42-b07c-829d741ad0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math  # for operating with NaNs\n",
        "\n",
        "# extract the sentences & labels of training set as numpy ndarrays\n",
        "sent_quest_1 = {}    # for saving all first sentences & questions\n",
        "sent_quest_2 = {}    # for saving all second sentences & questions are saved\n",
        "labels = {}          # for saving all labels & scores\n",
        "\n",
        "# print all colnames for each dataset in an ordered manner\n",
        "for i in data_names:\n",
        "  print('\\n' + i + ' colnames:')\n",
        "  for j in colnames[i]:\n",
        "    print(j)\n",
        "\n",
        "# print colnames of respective datasets (defines earlier) to find out which datasets\n",
        "# contain sentence1, etc.\n",
        "for i in data_names:\n",
        "  for j in colnames[i]:\n",
        "    # fct. endswith so that only variables which refer to sentence1/question(1) are\n",
        "    # taken into consideration\n",
        "    if j.endswith('sentence1'):  \n",
        "      sent_quest_1[i] = datasets[i].sentence1.values\n",
        "    elif j.endswith('question1'):\n",
        "      sent_quest_1[i] = datasets[i].question1.values\n",
        "    elif j.endswith('question'):\n",
        "      sent_quest_1[i] = datasets[i].question.values\n",
        "    # MRPC \n",
        "    elif j == 'String1':\n",
        "      sent_quest_1[i] = datasets[i].String1.values\n",
        "    ### now for sentence2/question2\n",
        "    elif j.endswith('sentence2'):\n",
        "      sent_quest_2[i] = datasets[i].sentence2.values\n",
        "    elif j.endswith('question2'):\n",
        "      sent_quest_2[i] = datasets[i].question2.values\n",
        "    elif j == 'String2':\n",
        "      sent_quest_2[i] = datasets[i].String2.values\n",
        "    ### now for labels\n",
        "    elif j.endswith('label_coded'):\n",
        "      labels[i] = datasets[i].label_coded.values\n",
        "    # MRPC\n",
        "    elif j == 'Quality':\n",
        "      labels[i] = datasets[i].Quality.values\n",
        "    elif j.endswith('score'):\n",
        "      labels[i] = datasets[i].score.values\n",
        "\n",
        "# for special cases (with only sentence) if statement outside of for loop necessary\n",
        "# for QNLI (question, sentence) and CoLA & SST-2 (both: only sentence)\n",
        "sent_quest_1['CoLA'] = datasets['CoLA'].sentence.values\n",
        "sent_quest_1['SST-2'] = datasets['SST-2'].sentence.values\n",
        "sent_quest_2['QNLI'] = datasets['QNLI'].sentence.values\n",
        "\n",
        "# print keys of labels\n",
        "print('\\n' + 'Keys of labels (to find out which are missing)')\n",
        "print(labels.keys())\n",
        "\n",
        "# for label (not encoded) if statement outside of loop (CoLA, SST-2, WNLI)\n",
        "labels['CoLA'] = datasets['CoLA'].label.values\n",
        "labels['SST-2'] = datasets['SST-2'].label.values\n",
        "labels['WNLI'] = datasets['WNLI'].label.values\n",
        "\n",
        "# for QQP (special case because didn't recognize that it's type int though it only \n",
        "# has values 0 and 1)\n",
        "labels['QQP'] = datasets['QQP'].is_duplicate.values.astype(np.int)\n",
        "\n",
        "print('\\n' + 'Keys of labels')\n",
        "print(labels.keys())\n",
        "\n",
        "# print dictionary keys\n",
        "print('\\n' + 'Keys of sent_quest_1')\n",
        "print(sent_quest_1.keys())\n",
        "print('\\n' + 'Keys of sent_quest_2')\n",
        "print(sent_quest_2.keys())\n",
        "\n",
        "##### Delete NaNs for QQP & MNLI!!! ###### \n",
        "# print  length of QQP sentences to compare later\n",
        "print('\\n' + 'Number of elements in QQP: ' + str(len(sent_quest_1['QQP'])))\n",
        "# create list to save nans\n",
        "nas = (np.argwhere(np.isnan(labels['QQP'])))\n",
        "nans_qqp = list()\n",
        "for na in nas:\n",
        "    nans_qqp.extend(na)\n",
        "# number of NANs\n",
        "print('\\n' + 'Number of NaNs in QQP: ' + str(len(nans_qqp)))\n",
        "# delete NaNs\n",
        "sent_quest_1['QQP'] = np.delete(sent_quest_1['QQP'], nans_qqp)\n",
        "sent_quest_2['QQP'] = np.delete(sent_quest_2['QQP'], nans_qqp)\n",
        "labels['QQP'] = np.delete(labels['QQP'], nans_qqp)\n",
        "# print  length of QQP sentences to compare \n",
        "print('\\n' + 'Number of elements in QQP: ' + str(len(sent_quest_1['QQP'])))\n",
        "\n",
        "### MNLI\n",
        "print('\\n' + 'Number of elements in MNLI: ' + str(len(sent_quest_1['MNLI'])))\n",
        "# create list to save nans\n",
        "nans_mnli = list()\n",
        "for i in range(0,len(sent_quest_2['MNLI'])):\n",
        "  if type(sent_quest_2['MNLI'][i]) != str:\n",
        "    nans_mnli.append(i)\n",
        "# Number of NaNs\n",
        "print('\\n' + 'Number of NaNs in MNLI: ' + str(len(nans_mnli)))\n",
        "# delete NaNs\n",
        "sent_quest_1['MNLI'] = np.delete(sent_quest_1['MNLI'], nans_mnli)\n",
        "sent_quest_2['MNLI'] = np.delete(sent_quest_2['MNLI'], nans_mnli)\n",
        "labels['MNLI'] = np.delete(labels['MNLI'], nans_mnli)\n",
        "print('\\n' + 'Number of elements in MNLI: ' + str(len(sent_quest_1['MNLI'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CoLA colnames:\n",
            "sentence_source\n",
            "label\n",
            "label_notes\n",
            "sentence\n",
            "\n",
            "MNLI colnames:\n",
            "index\n",
            "promptID\n",
            "pairID\n",
            "genre\n",
            "sentence1_binary_parse\n",
            "sentence2_binary_parse\n",
            "sentence1_parse\n",
            "sentence2_parse\n",
            "sentence1\n",
            "sentence2\n",
            "label1\n",
            "gold_label\n",
            "label_coded\n",
            "\n",
            "MRPC colnames:\n",
            "Quality\n",
            "ID1\n",
            "ID2\n",
            "String1\n",
            "String2\n",
            "\n",
            "QNLI colnames:\n",
            "index\n",
            "question\n",
            "sentence\n",
            "label\n",
            "label_coded\n",
            "\n",
            "QQP colnames:\n",
            "id\n",
            "qid1\n",
            "qid2\n",
            "question1\n",
            "question2\n",
            "is_duplicate\n",
            "\n",
            "RTE colnames:\n",
            "index\n",
            "sentence1\n",
            "sentence2\n",
            "label\n",
            "label_coded\n",
            "\n",
            "SST-2 colnames:\n",
            "sentence\n",
            "label\n",
            "\n",
            "STS-B colnames:\n",
            "index\n",
            "genre\n",
            "filename\n",
            "year\n",
            "old_index\n",
            "source1\n",
            "source2\n",
            "sentence1\n",
            "sentence2\n",
            "score\n",
            "\n",
            "WNLI colnames:\n",
            "index\n",
            "sentence1\n",
            "sentence2\n",
            "label\n",
            "\n",
            "Keys of labels (to find out which are missing)\n",
            "dict_keys(['MNLI', 'MRPC', 'QNLI', 'RTE', 'STS-B'])\n",
            "\n",
            "Keys of labels\n",
            "dict_keys(['MNLI', 'MRPC', 'QNLI', 'RTE', 'STS-B', 'CoLA', 'SST-2', 'WNLI', 'QQP'])\n",
            "\n",
            "Keys of sent_quest_1\n",
            "dict_keys(['MNLI', 'MRPC', 'QNLI', 'QQP', 'RTE', 'STS-B', 'WNLI', 'CoLA', 'SST-2'])\n",
            "\n",
            "Keys of sent_quest_2\n",
            "dict_keys(['MNLI', 'MRPC', 'QQP', 'RTE', 'STS-B', 'WNLI', 'QNLI'])\n",
            "\n",
            "Number of elements in QQP: 363870\n",
            "\n",
            "Number of NaNs in QQP: 0\n",
            "\n",
            "Number of elements in QQP: 363870\n",
            "\n",
            "Number of elements in MNLI: 392702\n",
            "\n",
            "Number of NaNs in MNLI: 40\n",
            "\n",
            "Number of elements in MNLI: 392662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QilChYCWavSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}