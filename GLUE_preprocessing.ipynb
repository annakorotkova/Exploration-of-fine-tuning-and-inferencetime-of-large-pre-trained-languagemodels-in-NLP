{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWe3y2QBuUtjap3Ty1sJ6t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annakorotkova/NLP_finetuning/blob/master/GLUE_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zTLe3NGXLBt",
        "colab_type": "code",
        "outputId": "421eb236-6e2a-45d3-e3df-8f50eb0ede9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cd5bb073bb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALG-jy3_XPBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie0OZTiEXRd8",
        "colab_type": "code",
        "outputId": "070d0826-017e-4c42-a78b-6919a8f73515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\r\u001b[K     |▌                               | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 358kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 368kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 378kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 389kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 399kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 409kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 419kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 430kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 440kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 450kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 460kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 471kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 481kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 491kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 501kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 512kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 522kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 532kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 542kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 552kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 563kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 573kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 583kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 593kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 604kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 614kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 624kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 634kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 645kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 655kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 665kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=629c23e7cbd858787d61b5959d31db160e91640ae0b0c1ef2cd2e5d0243eceed\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxgFNfQbqnW",
        "colab_type": "text"
      },
      "source": [
        "Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4iJNG3CcEks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Y1Gik3cFYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_data = drive.CreateFile({'id':'1AlbO4pMgbCAZO2XyGyKXsdWszyj-xMHx'})\n",
        "finetuning = drive.CreateFile({'id':'1GyndnqvwF_yCCrr0cX8et7sbXGOeRuP5'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz0-Ov4mboJX",
        "colab_type": "code",
        "outputId": "64328a97-6153-477d-e649-2da2b1963b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        }
      },
      "source": [
        "# Loading GLUE datasets notebook\n",
        "load_data.GetContentFile('Load_GLUE.ipynb')\n",
        "\n",
        "# run Load_GLUE notebook\n",
        "%run Load_GLUE.ipynb # this notebood downloads and extracts data\n",
        "\n",
        "# BERT finetuning notebook\n",
        "finetuning.GetContentFile('BERT_finetuning_function.ipynb') # this notebook contains the finetuning function\n",
        "\n",
        "# run BERT_finetuning notebook\n",
        "%run BERT_finetuning_function.ipynb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting SNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/content/BERT_finetuning_function.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiouTltQUS5o",
        "colab_type": "code",
        "outputId": "b082e988-caa6-4704-97d4-215de17caefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# names of respective datasets\n",
        "data_names = ['CoLA', 'MNLI', 'MRPC', 'QNLI', 'QQP', 'RTE', 'SST-2', 'STS-B', 'WNLI']\n",
        "# 'SNLI' also included, but not explicitly stated in GLUE benchmark --> not regarded until now\n",
        "\n",
        "\n",
        "# define dictionary to save column names for each dataset\n",
        "column_names = {}\n",
        "# to store all datasets in one dict\n",
        "datasets = {}\n",
        "\n",
        "# Load the datasets into a pandas dataframe.\n",
        "for i in data_names:\n",
        "  # for CoLA dataset, column names need to be added\n",
        "  if i == 'CoLA':\n",
        "    datasets[i] = pd.read_csv(\"/content/drive/My Drive/Master Thesis/{0}/train.tsv\".format(i), \n",
        "                    delimiter='\\\\t', quotechar='\"', \n",
        "                    names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "  else:\n",
        "    datasets[i] = pd.read_csv(\"/content/drive/My Drive/Master Thesis/{0}/train.tsv\".format(i), \n",
        "                    delimiter = '\\\\t', quotechar = '\"')\n",
        "  # Report the number of sentences.\n",
        "  print('Number of training sentences: {:,}\\n'.format(datasets[i].shape[0]))\n",
        "  # Display 5 random rows from the data.\n",
        "  print(datasets[i].sample(10))\n",
        "  # save column names for each dataset in dict\n",
        "  column_names[i] = datasets[i].columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n",
            "     sentence_source  ...                                           sentence\n",
            "7446           sks13  ...                      Mary thinks for Bill to come.\n",
            "2298            l-93  ...     I whipped the eggs from a puddle into a froth.\n",
            "171             cj99  ...  The more obnoxious Fred, the less attention yo...\n",
            "2861            l-93  ...               Carol carved the tree on the branch.\n",
            "8360            ad03  ...  When dining with evil crocodiles, it is advisa...\n",
            "5500            b_73  ...  You have presented so elegant a solution that ...\n",
            "6521            g_81  ...     The man who I think that chased Fido returned.\n",
            "2689            l-93  ...                        Amanda carried the package.\n",
            "2453            l-93  ...                There ran a little boy in the yard.\n",
            "102             cj99  ...    As you eat more, you want correspondingly less.\n",
            "\n",
            "[10 rows x 4 columns]\n",
            "Number of training sentences: 392,702\n",
            "\n",
            "         index  promptID  ...         label1     gold_label\n",
            "349484  349484     81475  ...        neutral        neutral\n",
            "247082  247082     74115  ...        neutral        neutral\n",
            "248409  248409    144698  ...  contradiction  contradiction\n",
            "369        369     28238  ...     entailment     entailment\n",
            "223188  223188     94681  ...     entailment     entailment\n",
            "369384  369384     53411  ...     entailment     entailment\n",
            "88390    88390      4459  ...  contradiction  contradiction\n",
            "145689  145689      5678  ...     entailment     entailment\n",
            "27181    27181    115042  ...     entailment     entailment\n",
            "100887  100887     72087  ...        neutral        neutral\n",
            "\n",
            "[10 rows x 12 columns]\n",
            "Number of training sentences: 3,668\n",
            "\n",
            "                                                                                                                Quality\n",
            "1 2269545 2269531 Abplanalp used plastic in a model that could be...  Mr. Abplanalp used plastic in a model that cou...\n",
            "  1590753 1590946 Tisha Kresler, a spokeswoman for Global Crossin...  A Global Crossing representative had no immedi...\n",
            "  767107  767069  But Close wondered whether the package would be...  Close also questions whether it would be worth...\n",
            "  2761139 2761113 In July, EMC agreed to acquire Legato Systems (...  In July, the Hopkinton, Mass., company agreed ...\n",
            "  3200940 3201393 Jackson, 45, posted a $3 million bond and retur...  After posting $3 million bail, Jackson flew to...\n",
            "0 987739  987595  Justice Clarence Thomas, joined by fellow conse...  Justice Antonin Scalia, Sandra Day O'Connor an...\n",
            "  3416795 3416666 He also wrote the state was right to execute a ...  He said he had told the state police to execut...\n",
            "  1741477 1742080 A view of the devastation left behind when a ma...  The body of a victim lies under a yellow tarp ...\n",
            "1 956131  956277  Dynes will get $395,000 a year, an increase ove...  In his new position, Dynes will earn $395,000,...\n",
            "0 377195  377203  Cox said state police would still help his offi...  On Friday, authorities had said the state poli...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 104,743\n",
            "\n",
            "         index  ...           label\n",
            "56370    56370  ...  not_entailment\n",
            "101810  101810  ...  not_entailment\n",
            "16081    16081  ...      entailment\n",
            "9716      9716  ...      entailment\n",
            "11870    11870  ...      entailment\n",
            "60127    60127  ...  not_entailment\n",
            "29830    29830  ...      entailment\n",
            "63362    63362  ...  not_entailment\n",
            "72160    72160  ...      entailment\n",
            "73708    73708  ...  not_entailment\n",
            "\n",
            "[10 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 363,870\n",
            "\n",
            "            id  ... is_duplicate\n",
            "296309  286902  ...          1.0\n",
            "182349  100679  ...          1.0\n",
            "96075   337516  ...          1.0\n",
            "180524   50066  ...          1.0\n",
            "190529   29163  ...          1.0\n",
            "87023   266090  ...          0.0\n",
            "268851  197063  ...          1.0\n",
            "17399   257439  ...          0.0\n",
            "198613  386782  ...          0.0\n",
            "262981  273433  ...          1.0\n",
            "\n",
            "[10 rows x 6 columns]\n",
            "Number of training sentences: 2,490\n",
            "\n",
            "      index  ...           label\n",
            "1478   1478  ...      entailment\n",
            "2055   2055  ...      entailment\n",
            "1357   1357  ...      entailment\n",
            "20       20  ...  not_entailment\n",
            "549     549  ...  not_entailment\n",
            "1784   1784  ...      entailment\n",
            "1950   1950  ...  not_entailment\n",
            "1882   1882  ...      entailment\n",
            "1232   1232  ...      entailment\n",
            "925     925  ...  not_entailment\n",
            "\n",
            "[10 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 67,349\n",
            "\n",
            "                                                sentence  label\n",
            "15807               movies , television and the theater       1\n",
            "20989  have been worth cheering as a breakthrough but...      0\n",
            "59549  is n't that stealing harvard is a horrible mov...      0\n",
            "60720                                               old       0\n",
            "38214                                serenity and poise       1\n",
            "61255  he probably would n't be too crazy with his gr...      0\n",
            "35708                           without being memorable       0\n",
            "44360                 overflows with wisdom and emotion       1\n",
            "9155                    israeli/palestinian conflict as       1\n",
            "54455  allen shows he can outgag any of those young w...      1\n",
            "Number of training sentences: 5,749\n",
            "\n",
            "      index  ... score\n",
            "1437   1437  ...  3.80\n",
            "4431   4431  ...  3.40\n",
            "3098   3098  ...  3.25\n",
            "760     760  ...  0.00\n",
            "304     304  ...  1.00\n",
            "5571   5571  ...  3.00\n",
            "1496   1496  ...  2.00\n",
            "2236   2236  ...  3.10\n",
            "1958   1958  ...  4.20\n",
            "3879   3879  ...  3.60\n",
            "\n",
            "[10 rows x 10 columns]\n",
            "Number of training sentences: 635\n",
            "\n",
            "     index  ... label\n",
            "41      41  ...     0\n",
            "107    107  ...     1\n",
            "61      61  ...     1\n",
            "135    135  ...     0\n",
            "168    168  ...     1\n",
            "544    544  ...     1\n",
            "174    174  ...     1\n",
            "395    395  ...     0\n",
            "249    249  ...     1\n",
            "243    243  ...     1\n",
            "\n",
            "[10 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izgiKkFsnfKA",
        "colab_type": "code",
        "outputId": "648b5c8e-0ffd-4092-db6e-e6904f8b4c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# print key & values of dictionary 'datasets'\n",
        "# for key, value in datasets.items():\n",
        "#    print (key, value)\n",
        "\n",
        "# save colnames of respective datasets in dictionary\n",
        "colnames = {}\n",
        "for i in data_names:\n",
        "    colnames[i] = datasets[i].columns.values\n",
        "\n",
        "print(colnames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CoLA': array(['sentence_source', 'label', 'label_notes', 'sentence'],\n",
            "      dtype=object), 'MNLI': array(['index', 'promptID', 'pairID', 'genre', 'sentence1_binary_parse',\n",
            "       'sentence2_binary_parse', 'sentence1_parse', 'sentence2_parse',\n",
            "       'sentence1', 'sentence2', 'label1', 'gold_label'], dtype=object), 'MRPC': array(['Quality'], dtype=object), 'QNLI': array(['index', 'question', 'sentence', 'label'], dtype=object), 'QQP': array(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'],\n",
            "      dtype=object), 'RTE': array(['index', 'sentence1', 'sentence2', 'label'], dtype=object), 'SST-2': array(['sentence', 'label'], dtype=object), 'STS-B': array(['index', 'genre', 'filename', 'year', 'old_index', 'source1',\n",
            "       'source2', 'sentence1', 'sentence2', 'score'], dtype=object), 'WNLI': array(['index', 'sentence1', 'sentence2', 'label'], dtype=object)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNgu8X8r_1pb",
        "colab_type": "code",
        "outputId": "6d1f8cb3-42a2-4cee-e565-f11aecd2e30d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# print all possible values of labels\n",
        "for i in data_names:\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    if datasets[i].columns.values[j] == 'label':\n",
        "      print(i + \" labels contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].label)) \n",
        "    elif datasets[i].columns.values[j] == 'gold_label':\n",
        "      print(i + \" labels (gold_label) contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].gold_label)) \n",
        "    elif datasets[i].columns.values[j] == 'is_duplicate':\n",
        "      print(i + \" labels (is_duplicate) contain following possible values of labels: \")\n",
        "      print(pd.unique(datasets[i].is_duplicate)) \n",
        " \n",
        "# unique values of labels: [0 1] and ['not_entailment' 'entailment']\n",
        "# unique values of gold_label (MNLI): ['neutral' 'entailment' 'contradiction']\n",
        "# unique values of is_duplicate (QQP): [ 0.  1. nan]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CoLA labels contain following possible values of labels: \n",
            "[1 0]\n",
            "MNLI labels (gold_label) contain following possible values of labels: \n",
            "['neutral' 'entailment' 'contradiction']\n",
            "QNLI labels contain following possible values of labels: \n",
            "['not_entailment' 'entailment']\n",
            "QQP labels (is_duplicate) contain following possible values of labels: \n",
            "[ 0.  1. nan]\n",
            "RTE labels contain following possible values of labels: \n",
            "['not_entailment' 'entailment']\n",
            "SST-2 labels contain following possible values of labels: \n",
            "[0 1]\n",
            "WNLI labels contain following possible values of labels: \n",
            "[1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9_KL77HW0Wb",
        "colab_type": "code",
        "outputId": "b61ea9af-8fc9-4d2d-8f1a-4cbb975700cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# modifying labels for MNLI, QNLI and RTE (before: string labels), \n",
        "# so that the model can work with them (model cannot work with strings)\n",
        "labels = {}\n",
        "mnli_qnli_rte = ['MNLI', 'QNLI', 'RTE']\n",
        "\n",
        "#### Encode labels that have string values as int values, so that the model can\n",
        "#### cope with it\n",
        "\n",
        "# create empty ndarrays for QNLI and RTE labels\n",
        "for i in mnli_qnli_rte:\n",
        "  # for each colums in respective datasets\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    if datasets[i].columns.values[j] == 'label':\n",
        "      labels[i] = np.empty(shape=datasets[i].shape[0], dtype = int, order='C')\n",
        "        # check if the lengths of the empty arrays equal lengths of the respective datasets\n",
        "    elif datasets[i].columns.values[j] == 'gold_label':\n",
        "      labels[i] = np.empty(shape=datasets[i].shape[0], dtype = int, order='C')\n",
        "  print(len(labels[i]) == datasets[i].shape[0])  # output: TRUE\n",
        "\n",
        "# encode labels for MNLI, QNLI and RTE\n",
        "for i in mnli_qnli_rte:\n",
        "  # for each colums in respective datasets\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    counter = 0\n",
        "    # for column 'label'\n",
        "    if datasets[i].columns.values[j] == 'label':\n",
        "      # for each label in label (for QNLI & RTE)\n",
        "      for label in datasets[i].label:\n",
        "        # encode 'not_entailment' as 0 (for MNLI)\n",
        "        if label == 'not_entailment':\n",
        "          labels[i][counter] = 0\n",
        "        # encode 'entailment' as 1\n",
        "        elif label == 'entailment':\n",
        "          labels[i][counter] = 1\n",
        "        counter += 1\n",
        "    elif datasets[i].columns.values[j] == 'gold_label':\n",
        "      # for each label in gold_label\n",
        "      for label in datasets[i].gold_label:\n",
        "        # encode 'neutral' as 0\n",
        "        if label == 'neutral':\n",
        "          labels[i][counter] = 0\n",
        "        # encode 'entailment' as 1\n",
        "        elif label == 'entailment':\n",
        "          labels[i][counter] = 1\n",
        "        # encode 'contradiction' as -1\n",
        "        elif label == 'contradiction':\n",
        "          labels[i][counter] = -1\n",
        "        counter += 1\n",
        "  # print unique labels and respective count of each unique label\n",
        "  print(np.unique(labels[i], return_counts=True))\n",
        "\n",
        "# add new column to dataset (with coded labels)\n",
        "for i in mnli_qnli_rte:\n",
        "  datasets[i]['coded_lables'] = labels[i]\n",
        "  print(np.unique(datasets[i].coded_lables))\n",
        "  print(datasets[i].sample(5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "(array([-1,  0,  1]), array([130903, 130900, 130899]))\n",
            "(array([0, 1]), array([52366, 52377]))\n",
            "(array([0, 1]), array([1241, 1249]))\n",
            "[-1  0  1]\n",
            "         index  promptID   pairID  ...         label1     gold_label coded_lables\n",
            "114029  114029    107632  107632n  ...        neutral        neutral            0\n",
            "234829  234829    110779  110779c  ...  contradiction  contradiction           -1\n",
            "386750  386750     65180   65180n  ...        neutral        neutral            0\n",
            "322069  322069     74271   74271e  ...     entailment     entailment            1\n",
            "295631  295631     88242   88242e  ...     entailment     entailment            1\n",
            "\n",
            "[5 rows x 13 columns]\n",
            "[0 1]\n",
            "         index  ... coded_lables\n",
            "101443  101443  ...            1\n",
            "96053    96053  ...            1\n",
            "52          52  ...            0\n",
            "51521    51521  ...            0\n",
            "23454    23454  ...            0\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "[0 1]\n",
            "      index  ... coded_lables\n",
            "2347   2347  ...            1\n",
            "360     360  ...            1\n",
            "1218   1218  ...            1\n",
            "1663   1663  ...            1\n",
            "356     356  ...            0\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKTGD1AiALSJ",
        "colab_type": "code",
        "outputId": "ee6c3ec7-0865-450b-cfc3-200fd9f951a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# extract the sentences & labels of training set as numpy ndarrays\n",
        "\n",
        "# print colnames of respective datasets (defines earlier) to find out which datasets\n",
        "# contain sentence1, etc.\n",
        "for key, values in colnames:\n",
        "    if 'sentence1' in colnames[key]:\n",
        "      print(key)\n",
        "  # don't need an argument because the dictionary datasets is implemented as default\n",
        "\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "sentence1={}\n",
        "for i in data_names:\n",
        "  for j in range(0, len(datasets[i].columns.values)):\n",
        "    if datasets[i].columns.values[j] == 'sentence1':\n",
        "      sentence1[\"{0}_sentence1\".format(i.lower)] = datasets[i].sentence1.values        \n",
        "\n",
        "#print(type(sentence1[i]))\n",
        "######\n",
        "# extract the sentences & labels of training set as numpy ndarrays\n",
        "#import numpy as np\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "#sentence_one = rte.sentence1.values\n",
        "#sentence_two = rte.sentence2.values\n",
        "#labels = rte.coded_labels.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-3f8ac772fe6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'sentence1'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# don't need an argument because the dictionary datasets is implemented as default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'key'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGvspIZhbs-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}